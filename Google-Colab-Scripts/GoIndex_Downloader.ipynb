{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recursive_GoIndex_Downloader_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH7ZlsGKsQ0X"
      },
      "source": [
        "## Recursive GoIndex Downloader by atlonxp\n",
        "\n",
        "**Features**\n",
        "*   Recursive crawler (**atlonxp**)\n",
        "*   Download all folders and files in a given url (**atlonxp**)\n",
        "*   Download all folders and files in in sub-folders (**atlonxp**)\n",
        "*   Adaptive delay in fetching url (**atlonxp**)\n",
        "*   Store folders/files directly to your Google Drive (**pankaj260**)\n",
        "*   Folders and files exclusion filters (**atlonxp**)\n",
        "*   Download queue supported (**atlonxp**)\n",
        "*   Auto-domain URL detection (**atlonxp**)\n",
        "*   API-based GoIndex crawler (**atlonxp**, **ifvv**)\n",
        "*   Parallel/Multiple files downloader (**atlonxp**)\n",
        "*   Auto-skip password-protected folders (**cxu-fork**)\n",
        "\n",
        "**Version 2** - API-based crawler with paralled files downloader\n",
        "\n",
        "\t28 April 2020 (v2.4.0)\n",
        "\n",
        "\t+ added feature: curl download mode as default (we found sometime, requests.get caused a corrupted file)\n",
        "\t+ added feature: file size check. If not the same in the metadata, we force download\n",
        "\t+ added feature: double file size check. Once a file is downloaded, we re-check the it size with the metdadata\n",
        "\t+ revised time delay while crawling and downloading\n",
        "\t+ fixed major bugs when checking file size\n",
        "\n",
        "\t26 April 2020 (v2.3.3)\n",
        "\n",
        "\t+ added downloaded size information\n",
        "\n",
        "\t22 April 2020 (v2.3.2)\n",
        "\n",
        "\t+ added summary\n",
        "\t+ added Exception when file is unable to download\n",
        "\n",
        "\t21 April 2020 (v2.3.1)\n",
        "\t---------------------\n",
        "\tWhile crawling, fetching might cause errors sometime due to some quick requests or server is\n",
        "\tbusy. This problem has caused the eror in getting a json, so we re-fetch the url again (up\n",
        "\tto MAX_RETRY_CRAWLING) or until we found key \"files\" in the return response. Once retries is\n",
        "\treached the maximum and the key \"files\" is not found, so we ignore this link (return [])\n",
        "\n",
        "\tAt the end, if you find there is failure, just re-run the download section again. Unless you\n",
        "\tset OVERWITE = TRUE, all files will be re-downloaded\n",
        "\n",
        "\t+ added MAX_RETRY_CRAWLING (v2.3)\n",
        "\t+ fixed FILE_EXISTING_CHECK (stupid) bug\n",
        "\t+ added failure-links download task\n",
        "\n",
        "\t20 April 2020 (v2.2)\n",
        "\t---------------------\n",
        "\tSome sub-folders may be password-protected which will cause the error while crawling, so we\n",
        "\tskip this folder\n",
        "\n",
        "\t+ added auto-skip password-protected folder\n",
        "\n",
        "\t17 April 2020 (v2.1)\n",
        "    ---------------------\n",
        "\t+ fixed URL duplicated when crawling\n",
        "\t+ added search key 'files' function\n",
        "\n",
        "\t16 April 2020 (v2.0)\n",
        "    ---------------------\n",
        "\t+ crawler_v2:\n",
        "\t\t* API-based GoIndex crawler\n",
        "\t\t* Collecting all urls to be downloaded\n",
        "\t+ parallel downloader\n",
        "\t\t* TDQM progress bar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHZKD2eIrSWC"
      },
      "source": [
        "11# Mounting Google Drive, ignore this section if you don't want to\n",
        "# save on your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei98hPvrrlNb"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install requests tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki6wS0MCrnqC"
      },
      "source": [
        "# Import dependencies\n",
        "\n",
        "import json\n",
        "from json import JSONDecodeError\n",
        "\n",
        "import multiprocessing\n",
        "import os\n",
        "from pathlib import Path\n",
        "from random import randint\n",
        "from time import sleep\n",
        "from urllib import parse\n",
        "\n",
        "import requests\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ew87GUTHUu2p"
      },
      "source": [
        "SHOW_DOWNLOAD_PROGRESS = False\n",
        "OVERWRITE = False\n",
        "\n",
        "MIN_DELAY = 3\n",
        "MAX_DELAY = 5\n",
        "MAX_RETRY_CRAWLING = 5\n",
        "\n",
        "def check_exclusion(name, exclusions):\n",
        "    for exc in exclusions:\n",
        "        if exc in name:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def find(key, dictionary):\n",
        "    for k, v in dictionary.items():\n",
        "        if k == key:\n",
        "            yield v\n",
        "        elif isinstance(v, dict):\n",
        "            for result in find(key, v):\n",
        "                yield result\n",
        "        elif isinstance(v, list):\n",
        "            for d in v:\n",
        "                for result in find(key, d):\n",
        "                    yield result\n",
        "\n",
        "\n",
        "def crawler_v2(url, downloading_dict, path, level, exclusions, verbose=False):\n",
        "    # let slow down a bit\n",
        "    # sleep(randint(MIN_DELAY, MAX_DELAY))\n",
        "\n",
        "    url = parse.urlparse(url)\n",
        "    print(url.geturl())\n",
        "\n",
        "    try:\n",
        "        response_text = ''\n",
        "        retry = 0\n",
        "        while 'files' not in response_text:\n",
        "            retry += 1\n",
        "            if retry > MAX_RETRY_CRAWLING:\n",
        "                break\n",
        "            if retry > 1:\n",
        "                print('retry #{}'.format(retry), url.geturl())\n",
        "                sleep(randint(MIN_DELAY, MAX_DELAY))\n",
        "            data = {'password': task['password']} if 'password' in task else {}\n",
        "            response = requests.post(url.geturl(), data=data)\n",
        "            response_text = response.text\n",
        "        # print(response.text)\n",
        "        response_json = json.loads(response_text)\n",
        "    except JSONDecodeError:\n",
        "        sleep(randint(MIN_DELAY, MAX_DELAY))\n",
        "        print('- Data is missing! change a plan -')\n",
        "        print('- > use terminal CURL            -')\n",
        "        try:\n",
        "            response = os.popen(\"curl --globoff {} -d ''\".format(url.geturl())).read()\n",
        "            response_json = json.loads(response)\n",
        "        except Exception as e:\n",
        "            print('Nah, something went wrong!')\n",
        "            print(e.args())\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print('Nah, something went wrong!')\n",
        "        print(e.args())\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        files_dict = list(find('files', response_json))[0]\n",
        "    except Exception as e:\n",
        "        print('Cannot fine value for the key of \"files\", skip this link')\n",
        "        files_dict = {}\n",
        "\n",
        "    for file in files_dict:\n",
        "        name = file['name']\n",
        "\n",
        "        # if @name contains exclusion word, we ignore\n",
        "        if check_exclusion(name, exclusions):\n",
        "            continue\n",
        "\n",
        "        if 'folder' in file['mimeType']:\n",
        "            next_url = url.geturl() + parse.quote(name) + \"/\"\n",
        "            next_path = os.path.join(path, name)\n",
        "            downloading_dict = crawler_v2(next_url, downloading_dict, next_path, level + 1, exclusions, verbose)\n",
        "        else:\n",
        "            name = file['name']\n",
        "            domain_name = url.geturl()\n",
        "            if verbose:\n",
        "                print('  ' + name)\n",
        "            try:\n",
        "                downloading_dict.append({\n",
        "                    'folder': path,\n",
        "                    'filename': name,\n",
        "                    'filename_abs': os.path.join(path, name),\n",
        "                    'size': file['size'],\n",
        "                    'url': '{}{}{}'.format(domain_name, '/' if not domain_name.endswith('/') else '', parse.quote(name)),\n",
        "                })\n",
        "            except:\n",
        "                print('skipping', url.geturl() + parse.quote(name))\n",
        "                continue\n",
        "\n",
        "    # print(json.dumps(downloading_dict, indent=2), end='\\n\\n')\n",
        "    return downloading_dict\n",
        "\n",
        "\n",
        "def download_agent(task, OVERWRITE=OVERWRITE, METHOD='curl', SILENT=True):\n",
        "    folder = task['folder']\n",
        "    filename = task['filename']\n",
        "    filename_abs = task['filename_abs']\n",
        "    url = task['url']\n",
        "    size = int(task['size'])\n",
        "\n",
        "    result = {\n",
        "        'task': task\n",
        "    }\n",
        "\n",
        "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        if not os.path.exists(filename_abs) or OVERWRITE:\n",
        "            pass\n",
        "        else:\n",
        "            # file exists, check file size\n",
        "            if os.path.getsize(filename_abs) >= size:\n",
        "                result.update({ 'status': -1 })\n",
        "                task = None\n",
        "            else:\n",
        "                pass\n",
        "        retry = False\n",
        "        if task:\n",
        "            sleep(randint(3, 10))\n",
        "            if METHOD is 'curl':\n",
        "                command = \"curl --globoff '{}' --output '{}' {}\".format(\n",
        "                    task['url'],\n",
        "                    task['filename_abs'],\n",
        "                    '-s' if SILENT else ''\n",
        "                    )\n",
        "                os.popen(command).read()\n",
        "                # check filesize again\n",
        "                if os.path.getsize(filename_abs) >= size:\n",
        "                    result.update({ 'status': 0 })\n",
        "                else:\n",
        "                    # error --> 404, user rate limit, etc. --> put in failures\n",
        "                    # list for re-downloading\n",
        "                    retry = True\n",
        "            else:\n",
        "                r = requests.get(url, stream=True)\n",
        "                if r.status_code is 200:\n",
        "                    with open(filename_abs, 'ab+') as f:\n",
        "                        f.write(r.content)\n",
        "                    # check filesize again\n",
        "                    if os.path.getsize(filename_abs) >= size:\n",
        "                        result.update({ 'status': 0 })\n",
        "                    else:\n",
        "                        # error --> user rate limit, etc. --> put in failures\n",
        "                        # list for re-downloading\n",
        "                        result.update({ 'status': 1 })\n",
        "                else:\n",
        "                    # error --> 404\n",
        "                    result.update({ 'status': 1 })\n",
        "\n",
        "        if retry:\n",
        "            r = requests.get(url, stream=True)\n",
        "            if r.status_code is 200:\n",
        "                with open(filename_abs, 'ab+') as f:\n",
        "                    f.write(r.content)\n",
        "                # check filesize again\n",
        "                if os.path.getsize(filename_abs) >= size:\n",
        "                    result.update({ 'status': 0 })\n",
        "                else:\n",
        "                    # error --> user rate limit, etc. --> put in failures\n",
        "                    # list for re-downloading\n",
        "                    result.update({ 'status': 1 })\n",
        "            else:\n",
        "                # error --> 404\n",
        "                result.update({ 'status': 1 })\n",
        "\n",
        "    except Exception as e:\n",
        "        print('[Exception]', e.args, task['url'])\n",
        "        result.update({ 'status': 1 })\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_filesize(size, power=3):\n",
        "    return size/pow(1024, power)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqwqZ8Qrr-6m",
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        }
      },
      "source": [
        "exclusions = ['__MACOSX/']\n",
        "# google drive destination link\n",
        "destination = \"/content/drive/Shared drives/Permanent Drive XXX\"\n",
        "download_tasks = [\n",
        "    {\n",
        "        'folder': 'JAVA Abdul bari',\n",
        "     # link of google index\n",
        "        'url': 'https://exclusive.getstudyfever.workers.dev/'\n",
        "    },\n",
        "]\n",
        "\n",
        "print('##################################')\n",
        "print('# Crawling all downloadable urls #')\n",
        "print('##################################', end='\\n\\n')\n",
        "tasks = []\n",
        "for task in download_tasks:\n",
        "    tasks += crawler_v2(task['url'], [], os.path.join(destination, task['folder']), 0, exclusions, verbose=False)\n",
        "    # print(json.dumps(tasks, indent=2), end='\\n\\n')\n",
        "\n",
        "total_size = get_filesize(sum([int(task['size']) for task in tasks]))\n",
        "\n",
        "# print(json.dumps(tasks, indent=2))\n",
        "print('\\nTotal Task:', len(tasks))\n",
        "print('Total size: %.3fGB' % total_size, end='\\n\\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjpVT5B_to7l"
      },
      "source": [
        "MAX_DOWNLOAD_TASKS = 32\n",
        "\n",
        "print('######################################')\n",
        "print('# Downloading {} files and folders  #'.format(len(tasks)))\n",
        "print('######################################', end='\\n\\n')\n",
        "pool = multiprocessing.Pool(processes=MAX_DOWNLOAD_TASKS)  # Num of CPUs\n",
        "\n",
        "downloads = []\n",
        "skips = []\n",
        "failures = []\n",
        "errors = []\n",
        "downloaded_size = 0\n",
        "with tqdm.tqdm(total=len(tasks)) as pbar:\n",
        "    for i, result in enumerate(pool.imap_unordered(download_agent, tasks)):\n",
        "        if result is not None:\n",
        "            status = result.get('status')\n",
        "            task = result.get('task')\n",
        "            downloaded = int(task['size'])\n",
        "            if status == 0:\n",
        "                downloaded_size += downloaded\n",
        "                pbar.set_description('[%.3f/%.3f GB] Downloading %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                downloads.append(task)\n",
        "            elif status == 1:\n",
        "                pbar.set_description('[%.3f/%.3f GB] Fail downloading %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                failures.append(task)\n",
        "            elif status == -1:\n",
        "                downloaded_size += downloaded\n",
        "                pbar.set_description('[%.3f/%.3f GB] Skipping %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                skips.append(task)\n",
        "            else:\n",
        "                pass\n",
        "        else:\n",
        "            pbar.set_description('[%.3f/%.3f GB] ERROR')\n",
        "        pbar.update()\n",
        "\n",
        "print('Waiting 1 minute')\n",
        "sleep(60)\n",
        "\n",
        "# print(json.dumps(failures, indent=2))\n",
        "if len(failures) > 0:\n",
        "    print('\\n\\n##################################')\n",
        "    print('# Retry all {} failures          #'.format(len(failures)))\n",
        "    print('##################################')\n",
        "    with tqdm.tqdm(total=len(failures)) as pbar:\n",
        "        for i, result in enumerate(pool.imap_unordered(download_agent, failures)):\n",
        "            if result is not None:\n",
        "                status = result.get('status')\n",
        "                task = result.get('task')\n",
        "                downloaded = int(task['size'])\n",
        "                if status == 0:\n",
        "                    downloaded_size += downloaded\n",
        "                    pbar.set_description('[%.3f/%.3f GB] Downloading %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                    downloads.append(task)\n",
        "                    del failures[failures.index(task)]\n",
        "                elif status == 1:\n",
        "                    pbar.set_description('[%.3f/%.3f GB] Fail downloading %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                elif status == -1:\n",
        "                    downloaded_size += downloaded\n",
        "                    pbar.set_description('[%.3f/%.3f GB] Skipping %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                    skips.append(task)\n",
        "                else:\n",
        "                    pass\n",
        "            pbar.update()\n",
        "\n",
        "pool.close()\n",
        "pool.terminate()\n",
        "\n",
        "print('\\n\\n##################################')\n",
        "print('# Summary                        #')\n",
        "print('##################################')\n",
        "print('Tasks     :', len(tasks))\n",
        "print('-----------------')\n",
        "print('Downloads :', len(downloads))\n",
        "print('Skip      :', len(skips))\n",
        "print('Failures  :', len(failures))\n",
        "print('Errors    :', len(errors))\n",
        "\n",
        "if len(failures) > 0:\n",
        "    print('\\nWait for 1 minute and Re-run this section again to download all fail tasks')\n",
        "    for f in failures:\n",
        "        print(f['url'])\n",
        "\n",
        "if len(errors) > 0:\n",
        "    print('\\nError tasks')\n",
        "    for f in errors:\n",
        "        print(f['url'])\n",
        "\n",
        "print('\\nAll done, Voila!')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}